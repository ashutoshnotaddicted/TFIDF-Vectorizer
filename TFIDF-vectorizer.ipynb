{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"OnV82tg1xLi0"},"source":["### Corpus"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"bUsYm9wjxLi1"},"outputs":[],"source":["## SkLearn# Collection of string documents\n","\n","corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eLwmFZfKxLi4"},"source":["### SkLearn Implementation"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"Np4dfQOkxLi4"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(corpus)\n","skl_output = vectorizer.transform(corpus)"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"-7Om8YpYxLi6","outputId":"0a3bd0f5-4424-4400-944f-4482a80bd799"},"outputs":[{"name":"stdout","output_type":"stream","text":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"]}],"source":["# sklearn feature names, they are sorted in alphabetic order by default.\n","\n","print(vectorizer.get_feature_names())"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"dTKplK96xLi-","outputId":"53722fa2-6756-4aa0-f179-37b578bb6890"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n"," 1.         1.91629073 1.        ]\n"]}],"source":["# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n","# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n","\n","print(vectorizer.idf_)"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"-CTiWHygxLjA","outputId":"8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"},"outputs":[{"data":{"text/plain":["(4, 9)"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["# shape of sklearn tfidf vectorizer output after applying transform method.\n","\n","skl_output.shape"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"bDKEpbA-xLjD","outputId":"87dafd65-5313-443f-8c6e-1b05cc8c2543"},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 8)\t0.38408524091481483\n","  (0, 6)\t0.38408524091481483\n","  (0, 3)\t0.38408524091481483\n","  (0, 2)\t0.5802858236844359\n","  (0, 1)\t0.46979138557992045\n"]}],"source":["# sklearn tfidf values for first line of the above corpus.\n","# Here the output is a sparse matrix\n","\n","print(skl_output[0])"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"3QWo34hexLjF","outputId":"cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 8)\t0.38408524091481483\n","  (0, 6)\t0.38408524091481483\n","  (0, 3)\t0.38408524091481483\n","  (0, 2)\t0.5802858236844359\n","  (0, 1)\t0.46979138557992045\n","==================================================\n","[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"]}],"source":["# sklearn tfidf values for first line of the above corpus.\n","# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n","# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n","\n","print(skl_output[0])\n","print('='*50)\n","print(skl_output[0].toarray())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qfIwx5LzxLjI"},"source":["### Your custom implementation"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"HjuCcJwXxLjJ"},"outputs":[],"source":["# Write your code here.\n","# Make sure its well documented and readble with appropriate comments.\n","# Compare your results with the above sklearn tfidf vectorizer\n","# You are not supposed to use any other library apart from the ones given below\n","\n","from collections import Counter\n","from tqdm import tqdm\n","from scipy.sparse import csr_matrix\n","import math\n","import operator\n","from sklearn.preprocessing import normalize\n","import numpy\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["def fit(data1):\n","    unq_words = set()\n","    if isinstance(data1, (list)):\n","        for row in data1:\n","            for wrd in row.split(\" \"):\n","                if len(wrd) < 2:\n","                    continue\n","                unq_words.add(wrd)\n","        unq_words = sorted(list(unq_words))\n","        worde = list(enumerate(unq_words))\n","        vocab_dict = {}\n","        for i in range(len(worde)):\n","            vocab_dict[worde[i][1]] = worde[i][0]\n","        return vocab_dict\n","    else:\n","        print(\"pass list of sentence\")"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"]}],"source":["vocab = fit(corpus)\n","print(vocab)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The obtained vocab is same as get_feature_names"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'and': 1.916290731874155, 'document': 1.2231435513142097, 'first': 1.5108256237659907, 'is': 1.0, 'one': 1.916290731874155, 'second': 1.916290731874155, 'the': 1.0, 'third': 1.916290731874155, 'this': 1.0}\n"]}],"source":["import math\n","corpus = [\n","     'this is the first document',\n","     'this document is the second document',\n","     'and this is the third one',\n","     'is this the first document',\n","]\n","def idf(x):\n","    idf_val = {}\n","    count_dict = {}\n","    for w in list(vocab.keys()):\n","        count_d = 0\n","        for i in x:\n","            if w in i.split():\n","                count_d = count_d+1\n","        count_dict[w] = count_d\n","        #print(count_dict[w])\n","        idf_val[w] = 1 + math.log((1+len(x))/(1+count_dict[w]))\n","    #print(count_dict)\n","    return idf_val\n","\n","print(idf(corpus))\n","\n","def tf(y):\n","    tf_val = {}\n","    for row in y:\n","        wrd_frq = dict(Counter(row.split()))\n","        for w in list(vocab.keys()):\n","            if w in wrd_frq.keys():\n","                tf = wrd_frq[w]/(sum(wrd_frq.values()))\n","                tf_val[w] = tf\n","    #print(tf_val)\n","    return tf_val\n","\n","#print(tf(corpus))\n","            \n","            \n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["idf values are same as obtained from vectorizer.idf_"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["def transform(data1, vocab):\n","    srow = []\n","    scolumn = []\n","    svalue = []\n","    if isinstance(data1, list):\n","        for indx,row in enumerate(tqdm(data1)):\n","            wrd_frq = dict(Counter(row.split()))\n","            for wrd,frq in wrd_frq.items():\n","                if len(wrd)<2:\n","                    continue\n","                col_indx = vocab.get(wrd, -2)\n","                \n","                if col_indx != -2:\n","                    tf_idf = tf(corpus)[wrd]*idf(corpus)[wrd]\n","                    srow.append(indx)\n","                    scolumn.append(col_indx)\n","                    svalue.append(tf_idf)\n","        \n","        return csr_matrix((svalue, (srow, scolumn)), shape=(len(data1), len(vocab)))\n","    else:\n","        print(\"need to pass list of strings\")                 \n","   "]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [00:00<00:00, 1000.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  (0, 1)\t0.4697913855799205\n","  (0, 2)\t0.580285823684436\n","  (0, 3)\t0.3840852409148149\n","  (0, 6)\t0.3840852409148149\n","  (0, 8)\t0.3840852409148149\n","==================================================\n","[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n","  0.38408524 0.         0.38408524]]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n","#           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n","vocab = fit(corpus)\n","#print(list(vocab.keys()))\n","m = transform(corpus, vocab)\n","#print(m)\n","print(normalize(m, norm='l2')[0])\n","#print(m.toarray())\n","print('='*50)\n","\n","print(normalize(m, norm='l2')[0].toarray())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["the values are same as obtained from skl_output[0].toarray()"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"NHxPLlwNxLjL","outputId":"9abd8e08-0e24-4975-9a13-4d3636d60323"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents in corpus =  746\n"]}],"source":["# Below is the code to load the cleaned_strings pickle file provided\n","# Here corpus is of list type\n","\n","import pickle\n","with open(r'C:\\Users\\HP\\OneDrive\\Applied ai\\Module 3\\assignment\\Implementing TFIDF vectorizer\\cleaned_strings', 'rb') as f:\n","    corpus1 = pickle.load(f)\n","    \n","# printing the length of the corpus loaded\n","print(\"Number of documents in corpus = \",len(corpus1))"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"ZULfoOIdxLjQ"},"outputs":[],"source":["# Write your code here.\n","# Try not to hardcode any values.\n","# Make sure its well documented and readble with appropriate comments."]},{"cell_type":"code","execution_count":69,"metadata":{"id":"1_DJnnR3xLjR"},"outputs":[],"source":["def fit(data1):\n","    unq_words = set()\n","    if isinstance(data1, (list)):\n","        for row in data1:\n","            for wrd in row.split(\" \"):\n","                if len(wrd) < 2:\n","                    continue\n","                unq_words.add(wrd)\n","        unq_words = sorted(list(unq_words))\n","        worde = list(enumerate(unq_words))\n","        vocab_dict = {}\n","        for i in range(len(worde)):\n","            vocab_dict[worde[i][1]] = worde[i][0]\n","        return vocab_dict\n","    else:\n","        print(\"pass list of sentence\")"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["vocab = fit(corpus1)\n","#print(vocab)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["def idf(x):\n","    idf_val = {}\n","    count_dict = {}\n","    for w in list(vocab.keys()):\n","        count_d = 0\n","        for i in x:\n","            if w in i.split():\n","                count_d = count_d+1\n","        count_dict[w] = count_d\n","        #print(count_dict[w])\n","        idf_val[w] = 1 + math.log((1+len(x))/(1+count_dict[w]))\n","    #print(count_dict)\n","    return idf_val\n","\n","#print(idf(corpus1))\n","\n","\n","            \n","            \n"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'aailiyah': 0, 'abandoned': 1, 'ability': 2, 'abroad': 3, 'absolutely': 4, 'abstruse': 5, 'abysmal': 6, 'academy': 7, 'accents': 8, 'accessible': 9, 'acclaimed': 10, 'accolades': 11, 'accurate': 12, 'accurately': 13, 'accused': 14, 'achievement': 15, 'achille': 16, 'ackerman': 17, 'act': 18, 'acted': 19, 'acting': 20, 'action': 21, 'actions': 22, 'actor': 23, 'actors': 24, 'actress': 25, 'actresses': 26, 'actually': 27, 'adams': 28, 'adaptation': 29, 'add': 30, 'added': 31, 'addition': 32, 'admins': 33, 'admiration': 34, 'admitted': 35, 'adorable': 36, 'adrift': 37, 'adventure': 38, 'advise': 39, 'aerial': 40, 'aesthetically': 41, 'affected': 42, 'affleck': 43, 'afraid': 44, 'africa': 45, 'afternoon': 46, 'age': 47, 'aged': 48, 'ages': 49}\n","{'aailiyah': 8.27482599910299, 'abandoned': 8.27482599910299, 'ability': 8.27482599910299, 'abroad': 8.27482599910299, 'absolutely': 8.27482599910299, 'abstruse': 8.27482599910299, 'abysmal': 8.27482599910299, 'academy': 8.27482599910299, 'accents': 8.27482599910299, 'accessible': 8.27482599910299, 'acclaimed': 8.27482599910299, 'accolades': 8.27482599910299, 'accurate': 8.27482599910299, 'accurately': 8.27482599910299, 'accused': 8.27482599910299, 'achievement': 8.27482599910299, 'achille': 8.27482599910299, 'ackerman': 8.27482599910299, 'act': 8.27482599910299, 'acted': 8.27482599910299, 'acting': 8.27482599910299, 'action': 8.27482599910299, 'actions': 8.27482599910299, 'actor': 8.27482599910299, 'actors': 8.27482599910299, 'actress': 8.27482599910299, 'actresses': 8.27482599910299, 'actually': 8.27482599910299, 'adams': 8.27482599910299, 'adaptation': 8.27482599910299, 'add': 8.27482599910299, 'added': 8.27482599910299, 'addition': 8.27482599910299, 'admins': 8.27482599910299, 'admiration': 8.27482599910299, 'admitted': 8.27482599910299, 'adorable': 8.27482599910299, 'adrift': 8.27482599910299, 'adventure': 8.27482599910299, 'advise': 8.27482599910299, 'aerial': 8.27482599910299, 'aesthetically': 8.27482599910299, 'affected': 8.27482599910299, 'affleck': 8.27482599910299, 'afraid': 8.27482599910299, 'africa': 8.27482599910299, 'afternoon': 8.27482599910299, 'age': 8.27482599910299, 'aged': 8.27482599910299, 'ages': 8.27482599910299}\n"]}],"source":["def idf_50(k):\n","    vocab_50_sort = {}\n","    idf_sort_50 = {}\n","    d_initial = idf(k)\n","    idf_sort = sorted(d_initial.items(), key = lambda x:x[1], reverse= True)\n","    idf_sort_50lst = idf_sort[0:50]\n","    #print(idf_sort_50lst)\n","    for i in range(len(idf_sort_50lst)):\n","        vocab_50_sort[idf_sort[i][0]] = vocab[idf_sort[i][0]]\n","        idf_sort_50[idf_sort[i][0]] = idf_sort[i][1]\n","    return vocab_50_sort, idf_sort_50\n","vocab_50,idfnew = idf_50(vocab)\n","print(vocab_50)\n","print(idfnew)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'acting': 0.1111111111111111, 'adorable': 0.09090909090909091, 'absolutely': 0.14285714285714285, 'actor': 0.14285714285714285, 'actors': 0.1, 'actually': 0.14285714285714285, 'addition': 0.09090909090909091, 'acted': 0.2, 'accused': 0.05555555555555555, 'afraid': 0.14285714285714285, 'advise': 0.5, 'affleck': 0.05555555555555555, 'age': 0.1, 'abstruse': 0.0014326647564469914, 'accurately': 0.0014326647564469914, 'action': 0.058823529411764705, 'actress': 0.25, 'admiration': 0.0014326647564469914, 'adrift': 0.0014326647564469914, 'aerial': 0.25, 'actresses': 0.1111111111111111, 'actions': 0.1, 'adventure': 0.3333333333333333, 'affected': 0.043478260869565216, 'abroad': 0.06666666666666667, 'admitted': 0.1111111111111111, 'admins': 0.16666666666666666, 'abandoned': 0.03225806451612903, 'afternoon': 0.125, 'aged': 0.07692307692307693, 'add': 0.08333333333333333, 'accolades': 0.07692307692307693, 'abysmal': 0.0024271844660194173, 'accents': 0.3333333333333333, 'africa': 0.0625, 'academy': 0.07142857142857142, 'adaptation': 0.0625, 'accessible': 0.2, 'achievement': 0.0024271844660194173, 'aailiyah': 0.09090909090909091, 'ability': 0.16666666666666666, 'adams': 0.047619047619047616, 'achille': 0.06666666666666667, 'acclaimed': 0.0625, 'ackerman': 0.0024271844660194173, 'act': 0.1111111111111111, 'ages': 0.0024271844660194173, 'aesthetically': 0.07692307692307693, 'accurate': 0.1111111111111111, 'added': 0.1111111111111111}\n"]}],"source":["def tf(y,vocab_50):\n","    tf_val = {}\n","    for row in y:\n","        wrd_frq = dict(Counter(row.split()))\n","        for w in list(vocab_50.keys()):\n","            if w in wrd_frq.keys():\n","                tf = wrd_frq[w]/(sum(wrd_frq.values()))\n","                tf_val[w] = tf\n","    #print(tf_val)\n","    return tf_val\n","tfnew = tf(corpus1,vocab_50)\n","print(tfnew)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["def transform(data1, vocab_50):\n","    srow = []\n","    scolumn = []\n","    svalue = []\n","    if isinstance(data1, (list,)):\n","        for indx,row in enumerate(tqdm(data1)):\n","            for wrd,frq in idfnew.items():\n","                if len(wrd)<2:\n","                    continue\n","                col_indx = vocab_50.get(wrd, -2)\n","\n","                \n","                if col_indx != -2:\n","                    tf_idf = tfnew[wrd]*idfnew[wrd]\n","                    srow.append(indx)\n","                    scolumn.append(col_indx)\n","                    svalue.append(tf_idf)\n","        \n","        return normalize((csr_matrix((svalue, (srow, scolumn)), shape=(len(data1), len(vocab_50)))), norm = 'l2')\n","    else:\n","        print(\"need to pass list of strings\")  \n","   "]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 746/746 [00:00<00:00, 14028.46it/s]"]},{"name":"stdout","output_type":"stream","text":["[[0.0898299  0.03187512 0.16468814 0.06587526 0.14116127 0.00141566\n","  0.00239837 0.07058063 0.32937629 0.19762577 0.06175805 0.07600991\n","  0.1097921  0.00141566 0.05489605 0.00239837 0.06587526 0.00239837\n","  0.1097921  0.19762577 0.1097921  0.05812523 0.09881289 0.14116127\n","  0.09881289 0.24703222 0.1097921  0.14116127 0.04705376 0.06175805\n","  0.08234407 0.1097921  0.0898299  0.16468814 0.00141566 0.1097921\n","  0.0898299  0.00141566 0.32937629 0.49406443 0.24703222 0.07600991\n","  0.04296212 0.05489605 0.14116127 0.06175805 0.12351611 0.09881289\n","  0.07600991 0.00239837]]\n","(1, 50)\n","(746, 50)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["vocab = fit(corpus1)\n","\n","n = transform(corpus1, vocab_50)\n","\n","print(n[0].toarray())\n","print(n[0].toarray().shape)\n","print(n.shape)\n","#print(n)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Assignment_3_Instructions.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.8 ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"}}},"nbformat":4,"nbformat_minor":0}
